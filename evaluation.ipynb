{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ml_collections \n",
    "import deepchest\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # load pretrained model for feature extraction\n",
    "        self.feature_extractor = torchvision.models.resnet50(pretrained=True)\n",
    "        # freeze feature extractor part\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # replace last layer with an indentity layer (to remove the last fc layer)\n",
    "        num_ftrs = self.feature_extractor.fc.in_features\n",
    "        self.feature_extractor.fc = nn.Identity(num_ftrs)\n",
    "\n",
    "        # embedding layer for position from sites inputs\n",
    "        embedding_dim = 8\n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=12, embedding_dim=embedding_dim)\n",
    "\n",
    "        # add new fc layers\n",
    "        self.fc1 = nn.Linear(num_ftrs + embedding_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, images, sites, masks):\n",
    "        # images is a tensor of batch_size x num_sites x 3 x 224 x 224\n",
    "\n",
    "        feature_vectors = []\n",
    "        for i in range(images.shape[1]):\n",
    "            each_image_site = images[:, i, :, :, :]\n",
    "            each_image_site = each_image_site.view(images.shape[0], images.shape[2], images.shape[3], images.shape[4])\n",
    "\n",
    "            # x is now batch_size x 3 x 224 x 224\n",
    "            x = self.feature_extractor(each_image_site)\n",
    "            feature_vectors.append(x)\n",
    "\n",
    "        # position embedding of size batch_size x num_sites x embedding_dim\n",
    "        embedded_sites = self.pos_embedding(sites)\n",
    "\n",
    "        # stack all feature vectors to a new dimension of size batch_size x num_sites x num_features (512 for ResNet18)\n",
    "        x = torch.stack(feature_vectors, dim=1)\n",
    "\n",
    "        # concatenate feature vectors and position embeddings (batch_size x num_sites x [num_features + embedding_dim])\n",
    "        x = torch.concat([x, embedded_sites], dim=2)\n",
    "\n",
    "\n",
    "        masks = masks == 1 # convert to boolean\n",
    "        masks = masks.unsqueeze(-1).expand(x.size()) # expand from batch_size x num_sites to batch_size x num_sites x num_features (512 for ResNet18)\n",
    "\n",
    "        x = x * masks # apply masks and preserver the original tensor dimensions\n",
    "\n",
    "        # average all feature vectors\n",
    "        x = torch.mean(x, dim=1) # batch_size x [num_features + embedding_dim]\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = ml_collections.ConfigDict()\n",
    "\n",
    "config.batch_size = 32\n",
    "config.num_steps = 300\n",
    "\n",
    "# See preprocessing.py, if you replace with \";\" no preprocessing is done\n",
    "config.preprocessing_train_eval = \"independent_dropout(.2);\"\n",
    "\n",
    "config.use_validation_split = False\n",
    "\n",
    "# If validation split is false, then train will have 4/5 of data and test 1/5\n",
    "# If validation split is true, then train will have 3/5 of data, test 1/5 and val 1/5\n",
    "config.num_folds = 5\n",
    "\n",
    "# gpu workers\n",
    "config.num_workers = 0\n",
    "\n",
    "# dataset\n",
    "config.images_directory = \"dataset/images.dataset2/\"\n",
    "config.labels_file = \"dataset/labels.dataset2/diagnostic.csv\"\n",
    "\n",
    "# Fold seed\n",
    "config.random_state = 0\n",
    "\n",
    "# Where the indices are saved\n",
    "config.save_dir = \"model_saved/\"\n",
    "config.export_folds_indices_file = \"indices.csv\"\n",
    "\n",
    "# Don't modify these (should not have been in the config)\n",
    "config.test_fold_index = 0\n",
    "config.delta_from_test_index_to_validation_index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "checkpoint = torch.load(os.path.join(\n",
    "            config.save_dir,\n",
    "            f\"best_model_resnet18_sigmoid_epoch{0}_test_fold_index{config.test_fold_index}.ds1\",\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 1 epoch 0: 0.6875\t(min 0.53125 index 0, max 0.84375 index 2)\n",
      "dataset 1 epoch 1: 0.706250011920929\t(min 0.625 index 1, max 0.875 index 0)\n",
      "dataset 1 epoch 2: 0.7749999761581421\t(min 0.65625 index 1, max 0.90625 index 4)\n",
      "dataset 1 epoch 3: 0.7250000238418579\t(min 0.59375 index 4, max 0.875 index 0)\n",
      "+++++++++++++\n",
      "dataset 2 epoch 0: 0.515625\t(min 0.46875 index 3, max 0.59375 index 1)\n",
      "dataset 2 epoch 1: 0.53125\t(min 0.5 index 0, max 0.59375 index 1)\n",
      "dataset 2 epoch 2: 0.5\t(min 0.46875 index 3, max 0.53125 index 0)\n",
      "dataset 2 epoch 3: 0.5859375\t(min 0.5 index 1, max 0.71875 index 2)\n",
      "+++++++++++++\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "for ds in [1, 2]:\n",
    "    for epoch in range(4):\n",
    "        checkpoint_path = os.path.join(\n",
    "            config.save_dir,\n",
    "            f\"best_model_resnet18_sigmoid_epoch{epoch}_test_fold_index*{'.ds'+str(ds)+'-1'}\"\n",
    "            # {'.ds'+str(ds)+'-1' if ds == 1 else ''}\",\n",
    "        )\n",
    "        list_checkpoints = glob.glob(checkpoint_path)\n",
    "        accs = []\n",
    "        for file in list_checkpoints:\n",
    "            checkpoint = torch.load(file)\n",
    "            accs.append(checkpoint['acc'])\n",
    "        min_acc = min([(a.data, i) for i, a in enumerate(accs)])[0]\n",
    "        min_idx = min([(a.data, i) for i, a in enumerate(accs)])[1]\n",
    "        max_acc = max([(a.data, i) for i, a in enumerate(accs)])[0]\n",
    "        max_idx = max([(a.data, i) for i, a in enumerate(accs)])[1]\n",
    "        print(f'dataset {ds} epoch {epoch}: {torch.mean(torch.Tensor(accs))}\\t(min {min_acc} index {min_idx}, max {max_acc} index {max_idx})')\n",
    "    print('+++++++++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
